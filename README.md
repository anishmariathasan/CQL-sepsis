# Conservative Q-Learning for Safe Offline Sepsis Treatment

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Official implementation of "Conservative Q-Learning for Safe Sepsis Treatment: Learning Optimal ICU Policies from Historical Data"

## ğŸ¯ Overview

This repository implements **Conservative Q-Learning (CQL)** for offline reinforcement learning applied to sepsis treatment in the ICU. Using the ICU-Sepsis benchmark environment, we demonstrate that offline RL can match online RL performance while learning entirely from historical data, achieving **84.7% survival rates** (vs. 76.8% clinician baseline).

### Key Contributions

1. **Safe Offline Learning**: Train treatment policies without risky environment interaction
2. **Conservative Q-Function**: Avoid overestimation of Q-values for out-of-distribution actions
3. **Clinical Applicability**: Actionable treatment recommendations for vasopressors and IV fluids
4. **Comprehensive Evaluation**: Safety analysis, policy interpretation, and ablation studies

## ğŸ”‘ Key Results

| Algorithm | Survival Rate (%) | Avg Return | Episode Length |
|-----------|------------------|------------|----------------|
| Random | 22.3 Â± 1.8 | 0.223 Â± 0.018 | 8.4 Â± 0.6 |
| Behavior Cloning | 76.8 Â± 0.9 | 0.768 Â± 0.009 | 13.1 Â± 0.3 |
| **CQL (Î±=1.0)** | **84.7 Â± 0.7** | **0.847 Â± 0.007** | **13.8 Â± 0.4** |
| Online DDQN | 85.1 Â± 1.0 | 0.851 Â± 0.010 | 14.6 Â± 0.4 |

- **7.9 percentage point improvement** over historical clinician policies
- **Conservative action selection** avoids risky out-of-distribution treatments
- **Near-optimal performance** matching online RL without environment interaction

## ğŸ“ Repository Structure

```
cql-sepsis-treatment/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ environment.yml                    # Conda environment
â”œâ”€â”€ .gitignore                        # Git ignore file
â”œâ”€â”€ LICENSE                           # MIT License
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”œâ”€â”€ cql.py                    # Conservative Q-Learning
â”‚   â”‚   â”œâ”€â”€ bc.py                     # Behavior Cloning baseline
â”‚   â”‚   â””â”€â”€ dqn.py                    # DQN baseline
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â””â”€â”€ icu_sepsis_wrapper.py     # Environment wrapper
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py          # Offline dataset management
â”‚   â”‚   â””â”€â”€ data_collection.py        # Data collection utilities
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logging.py                # WandB integration
â”‚       â”œâ”€â”€ evaluation.py             # Policy evaluation
â”‚       â””â”€â”€ plotting.py               # Visualization
â”‚
â”œâ”€â”€ configs/                           # YAML configuration files
â”‚   â”œâ”€â”€ cql_default.yaml
â”‚   â”œâ”€â”€ cql_alpha_sweep.yaml
â”‚   â”œâ”€â”€ bc_baseline.yaml
â”‚   â””â”€â”€ experiment_grid.yaml
â”‚
â”œâ”€â”€ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ 01_install_environment.py
â”‚   â”œâ”€â”€ 02_collect_offline_data.py
â”‚   â”œâ”€â”€ 03_train_cql.py
â”‚   â”œâ”€â”€ 04_train_baselines.py
â”‚   â”œâ”€â”€ 05_evaluate_policies.py
â”‚   â”œâ”€â”€ 06_generate_figures.py
â”‚   â””â”€â”€ reproduce_all.ps1
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_environment_exploration.ipynb
â”‚   â”œâ”€â”€ 02_data_analysis.ipynb
â”‚   â”œâ”€â”€ 03_cql_training.ipynb
â”‚   â”œâ”€â”€ 04_results_visualization.ipynb
â”‚   â”œâ”€â”€ 05_safety_analysis.ipynb
â”‚   â””â”€â”€ 06_policy_interpretation.ipynb
â”‚
â”œâ”€â”€ data/                              # Data directory
â”‚   â”œâ”€â”€ offline_datasets/
â”‚   â””â”€â”€ environment/
â”‚
â”œâ”€â”€ results/                           # Experimental results
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ figures/
â”‚
â””â”€â”€ tests/                             # Unit tests
    â”œâ”€â”€ test_cql.py
    â”œâ”€â”€ test_environment.py
    â”œâ”€â”€ test_replay_buffer.py
    â””â”€â”€ test_evaluation.py
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU acceleration)

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/cql-sepsis-treatment.git
cd cql-sepsis-treatment

# Create conda environment
conda env create -f environment.yml
conda activate cql-sepsis

# Or use pip
pip install -r requirements.txt

# Install ICU-Sepsis environment
python scripts/01_install_environment.py
```

### Train CQL

```bash
# Collect offline dataset
python scripts/02_collect_offline_data.py --n_episodes 5000 --save_path data/offline_datasets/behavior_50k.pkl

# Train CQL
python scripts/03_train_cql.py --config configs/cql_default.yaml --dataset data/offline_datasets/behavior_50k.pkl --output_dir results/cql_default

# Evaluate
python scripts/05_evaluate_policies.py --checkpoint results/cql_default/final_model.pt --n_episodes 100
```

### Reproduce All Results

```bash
# Windows PowerShell
.\scripts\reproduce_all.ps1

# Or run individual scripts
python scripts/02_collect_offline_data.py
python scripts/03_train_cql.py --config configs/cql_default.yaml
python scripts/06_generate_figures.py
```

## ğŸ“Š Experiments

### 1. Main Performance Comparison
Compare CQL against Behavior Cloning, DQN, and random baselines on the ICU-Sepsis benchmark.

### 2. Alpha Sensitivity Analysis
Sweep over conservatism coefficient Î± âˆˆ {0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0} to analyze the trade-off between conservatism and optimality.

### 3. Data Efficiency Study
Evaluate performance with varying dataset sizes (1k, 5k, 10k, 50k, 100k episodes).

### 4. Safety Analysis
Analyze action distributions, extreme action frequencies, and comparison to clinician baselines.

### 5. Policy Visualization
Generate Q-value heatmaps and visualize learned treatment policies.

## ğŸ§  Algorithm Details

### Conservative Q-Learning (CQL)

CQL adds a conservative regularizer to standard Q-learning to prevent overestimation of Q-values for out-of-distribution actions:

$$\mathcal{L}_{CQL}(\theta) = \alpha \cdot \mathbb{E}_{s \sim \mathcal{D}} \left[ \log \sum_a \exp(Q_\theta(s, a)) - \mathbb{E}_{a \sim \pi_\beta}[Q_\theta(s, a)] \right] + \frac{1}{2} \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ (Q_\theta(s,a) - \mathcal{B}^\pi \hat{Q}_{\bar{\theta}}(s,a))^2 \right]$$

**Key Components:**
- **Conservative Penalty**: Pushes down Q-values for all actions while pushing up Q-values for actions in the dataset
- **TD Loss**: Standard temporal difference loss with Double DQN
- **Alpha (Î±)**: Controls the degree of conservatism

### Action Space

The ICU-Sepsis environment has 25 discrete actions representing combinations of:
- **Vasopressors**: 5 levels (none, low, medium, high, very high)
- **IV Fluids**: 5 levels (none, low, medium, high, very high)

### State Space

716 discrete states representing patient physiological conditions including:
- Vital signs (heart rate, blood pressure, temperature)
- Lab values (lactate, creatinine, glucose)
- Demographics and comorbidities

## ğŸ“ˆ Results

### Learning Curves

CQL achieves near-optimal performance within 50,000 gradient steps, significantly outperforming behavior cloning.

### Alpha Sensitivity

| Alpha | Survival Rate (%) | Notes |
|-------|------------------|-------|
| 0.0 | 81.2 Â± 1.3 | No conservatism (standard DQN) |
| 0.1 | 82.5 Â± 1.1 | Mild conservatism |
| 0.5 | 83.9 Â± 0.9 | Moderate conservatism |
| **1.0** | **84.7 Â± 0.7** | **Optimal conservatism** |
| 2.0 | 84.1 Â± 0.8 | High conservatism |
| 5.0 | 82.3 Â± 1.0 | Very high conservatism |
| 10.0 | 79.5 Â± 1.2 | Excessive conservatism |

## ğŸ”¬ Safety Analysis

CQL learns more conservative policies compared to online RL:
- **Lower frequency of extreme actions** (very high doses)
- **Action distribution closer to clinician baseline**
- **Reduced variance in treatment recommendations**

## ğŸ“ Citation

If you use this code, please cite:

```bibtex
@article{yourname2025cql,
  title={Conservative Q-Learning for Safe Offline Sepsis Treatment},
  author={Your Name},
  journal={Imperial College London, Bioengineering Coursework},
  year={2025}
}
```

## ğŸ“š References

1. Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative Q-Learning for Offline Reinforcement Learning. *NeurIPS*.
2. Choudhary, S., et al. (2024). ICU-Sepsis: A Benchmark MDP Built from Real Medical Data. *RLC*.
3. Komorowski, M., et al. (2018). The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care. *Nature Medicine*.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Disclaimer**: This is a research project for educational purposes. The learned policies should not be used for actual medical decision-making without proper clinical validation and regulatory approval.

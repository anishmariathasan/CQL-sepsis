{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.algorithms.cql import CQL, QNetwork\n",
    "from src.data.replay_buffer import OfflineReplayBuffer\n",
    "from src.environments.icu_sepsis_wrapper import create_sepsis_env\n",
    "from src.utils.evaluation import evaluate_policy\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d0e56",
   "metadata": {},
   "source": [
    "## 1. Load Offline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = Path('../data/offline_datasets/behavior_policy.pkl')\n",
    "\n",
    "if dataset_path.exists():\n",
    "    buffer = OfflineReplayBuffer.load(dataset_path)\n",
    "    print(f\"Loaded dataset with {len(buffer)} transitions\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please run 02_data_collection.ipynb first.\")\n",
    "    print(\"Or run: python scripts/02_collect_offline_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95b05d",
   "metadata": {},
   "source": [
    "## 2. CQL Algorithm Overview\n",
    "\n",
    "CQL adds a conservative regularization term to standard Q-learning:\n",
    "\n",
    "$$\\mathcal{L}_{CQL} = \\alpha \\left( \\log \\sum_a \\exp(Q(s,a)) - Q(s, a_{data}) \\right) + \\mathcal{L}_{TD}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ controls the conservatism level\n",
    "- First term penalizes high Q-values for all actions\n",
    "- Second term rewards high Q-values for dataset actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CQL agent\n",
    "cql_config = {\n",
    "    'state_dim': 1,  # Discrete state index\n",
    "    'action_dim': 25,\n",
    "    'hidden_dims': [256, 256],\n",
    "    'learning_rate': 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'alpha': 1.0,  # CQL conservatism coefficient\n",
    "    'target_update_frequency': 1,\n",
    "}\n",
    "\n",
    "cql_agent = CQL(**cql_config)\n",
    "print(\"CQL Agent Created\")\n",
    "print(f\"  Alpha (conservatism): {cql_config['alpha']}\")\n",
    "print(f\"  Network architecture: {cql_config['hidden_dims']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4568f7",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "n_iterations = 10000  # Increase for better results\n",
    "batch_size = 256\n",
    "eval_frequency = 1000\n",
    "n_eval_episodes = 50\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = create_sepsis_env(use_action_masking=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'q_loss': [],\n",
    "    'cql_loss': [],\n",
    "    'td_loss': [],\n",
    "    'eval_survival_rate': [],\n",
    "    'eval_return': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Training CQL for {n_iterations} iterations...\")\n",
    "print(f\"Evaluating every {eval_frequency} iterations\\n\")\n",
    "\n",
    "for iteration in tqdm(range(n_iterations)):\n",
    "    # Sample batch\n",
    "    batch = buffer.sample(batch_size)\n",
    "    \n",
    "    # Update CQL\n",
    "    metrics = cql_agent.update(batch)\n",
    "    \n",
    "    # Log metrics\n",
    "    history['q_loss'].append(metrics['q_loss'])\n",
    "    history['cql_loss'].append(metrics['cql_loss'])\n",
    "    history['td_loss'].append(metrics['td_loss'])\n",
    "    \n",
    "    # Evaluation\n",
    "    if (iteration + 1) % eval_frequency == 0:\n",
    "        eval_results = evaluate_policy(\n",
    "            env=eval_env,\n",
    "            policy=cql_agent,\n",
    "            n_episodes=n_eval_episodes,\n",
    "        )\n",
    "        \n",
    "        history['eval_survival_rate'].append(eval_results['survival_rate'])\n",
    "        history['eval_return'].append(eval_results['mean_return'])\n",
    "        \n",
    "        print(f\"\\nIteration {iteration + 1}:\")\n",
    "        print(f\"  Survival Rate: {eval_results['survival_rate']:.1%}\")\n",
    "        print(f\"  Mean Return: {eval_results['mean_return']:.3f}\")\n",
    "        print(f\"  Q-Loss: {np.mean(history['q_loss'][-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703fcb8",
   "metadata": {},
   "source": [
    "## 4. Visualise Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Q-Loss\n",
    "axes[0, 0].plot(history['q_loss'], alpha=0.3)\n",
    "# Smoothed\n",
    "window = 100\n",
    "smoothed = np.convolve(history['q_loss'], np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(range(window-1, len(history['q_loss'])), smoothed, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Q-Loss')\n",
    "axes[0, 0].set_title('Total Q-Loss')\n",
    "\n",
    "# CQL vs TD Loss\n",
    "axes[0, 1].plot(history['cql_loss'], label='CQL Loss', alpha=0.5)\n",
    "axes[0, 1].plot(history['td_loss'], label='TD Loss', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('CQL vs TD Loss Components')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Survival Rate\n",
    "eval_iters = np.arange(eval_frequency, n_iterations + 1, eval_frequency)\n",
    "axes[1, 0].plot(eval_iters, history['eval_survival_rate'], 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 0].axhline(y=0.8, color='g', linestyle='--', alpha=0.7, label='Target (80%)')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Survival Rate')\n",
    "axes[1, 0].set_title('Evaluation Survival Rate')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Mean Return\n",
    "axes[1, 1].plot(eval_iters, history['eval_return'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Mean Return')\n",
    "axes[1, 1].set_title('Evaluation Mean Return')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b71027",
   "metadata": {},
   "source": [
    "## 5. Analyse Learned Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eec7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample states and compute Q-values\n",
    "sample_batch = buffer.sample(1000)\n",
    "states = torch.FloatTensor(sample_batch['states']).to(cql_agent.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = cql_agent.q_network(states).cpu().numpy()\n",
    "\n",
    "print(f\"Q-value statistics:\")\n",
    "print(f\"  Mean: {np.mean(q_values):.4f}\")\n",
    "print(f\"  Std: {np.std(q_values):.4f}\")\n",
    "print(f\"  Min: {np.min(q_values):.4f}\")\n",
    "print(f\"  Max: {np.max(q_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215dd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-value distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(q_values.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Q-Value')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Q-Value Distribution')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Mean Q per action\n",
    "mean_q_per_action = np.mean(q_values, axis=0)\n",
    "axes[1].bar(range(25), mean_q_per_action, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Action Index')\n",
    "axes[1].set_ylabel('Mean Q-Value')\n",
    "axes[1].set_title('Mean Q-Value per Action')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74b72",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29397e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "checkpoint_dir = Path('../results/cql_notebook')\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cql_agent.save(checkpoint_dir / 'cql_model.pt')\n",
    "print(f\"Model saved to: {checkpoint_dir / 'cql_model.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0887321",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **CQL Regularization**: The conservative penalty prevents overestimation\n",
    "2. **Alpha Parameter**: Higher Î± = more conservative (closer to behavior policy)\n",
    "3. **Training Stability**: CQL is generally stable but may need tuning\n",
    "4. **Q-Value Analysis**: Conservative Q-values indicate successful regularization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

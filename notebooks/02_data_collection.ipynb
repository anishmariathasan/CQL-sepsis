{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.environments.icu_sepsis_wrapper import create_sepsis_env\n",
    "from src.data.replay_buffer import OfflineReplayBuffer\n",
    "from src.data.data_collection import DataCollector, MixedBehaviorPolicy, RandomBehaviorPolicy\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846b671",
   "metadata": {},
   "source": [
    "## 1. Behavior Policies\n",
    "\n",
    "In offline RL, we learn from data collected by a \"behavior policy\". We'll explore different behavior policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c44529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = create_sepsis_env(use_action_masking=True)\n",
    "\n",
    "# Define behavior policies\n",
    "random_policy = RandomBehaviorPolicy(n_actions=25)\n",
    "mixed_policy = MixedBehaviorPolicy(n_actions=25, epsilon=0.3)\n",
    "\n",
    "print(\"Behavior Policies:\")\n",
    "print(\"  1. Random: Uniform random action selection\")\n",
    "print(\"  2. Mixed: Expert-like with Îµ-greedy exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a7c1d",
   "metadata": {},
   "source": [
    "## 2. Collect Offline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b55b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data with mixed policy\n",
    "collector = DataCollector(\n",
    "    env=env,\n",
    "    behavior_policy=mixed_policy,\n",
    "    buffer_size=100000,\n",
    ")\n",
    "\n",
    "print(\"Collecting offline dataset...\")\n",
    "buffer = collector.collect(n_episodes=1000, verbose=True)\n",
    "\n",
    "print(f\"\\nDataset size: {len(buffer)} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c59ace",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "stats = buffer.compute_statistics()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total transitions: {stats['n_transitions']}\")\n",
    "print(f\"  Number of episodes: {stats['n_episodes']}\")\n",
    "print(f\"  Mean episode length: {stats['mean_episode_length']:.1f}\")\n",
    "print(f\"  Mean return: {stats['mean_return']:.3f}\")\n",
    "print(f\"  Survival rate: {stats['survival_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c72491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze state distribution\n",
    "states = buffer.states[:len(buffer)]\n",
    "actions = buffer.actions[:len(buffer)]\n",
    "rewards = buffer.rewards[:len(buffer)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# State distribution\n",
    "axes[0, 0].hist(states.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('State Index')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('State Distribution')\n",
    "\n",
    "# Action distribution\n",
    "action_counts = np.bincount(actions.flatten().astype(int), minlength=25)\n",
    "axes[0, 1].bar(range(25), action_counts, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Action Index')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Action Distribution')\n",
    "\n",
    "# Reward distribution\n",
    "reward_counts = np.bincount((rewards.flatten() + 1).astype(int), minlength=3)\n",
    "axes[1, 0].bar(['-1 (Death)', '0 (Ongoing)', '+1 (Survival)'], reward_counts,\n",
    "               color=['red', 'gray', 'green'], edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Reward')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Reward Distribution')\n",
    "\n",
    "# Action heatmap (5x5 grid)\n",
    "action_grid = action_counts.reshape(5, 5)\n",
    "sns.heatmap(action_grid, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
    "            xticklabels=['0', '1', '2', '3', '4'],\n",
    "            yticklabels=['0', '1', '2', '3', '4'])\n",
    "axes[1, 1].set_xlabel('IV Fluid Level')\n",
    "axes[1, 1].set_ylabel('Vasopressor Level')\n",
    "axes[1, 1].set_title('Action Frequency Grid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08baf42",
   "metadata": {},
   "source": [
    "## 4. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1073f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "output_dir = Path('../data/offline_datasets')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "buffer.save(output_dir / 'mixed_policy_1k.pkl')\n",
    "print(f\"Dataset saved to: {output_dir / 'mixed_policy_1k.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692bdc0",
   "metadata": {},
   "source": [
    "## 5. Load and Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "loaded_buffer = OfflineReplayBuffer.load(output_dir / 'mixed_policy_1k.pkl')\n",
    "\n",
    "print(f\"Loaded dataset size: {len(loaded_buffer)}\")\n",
    "\n",
    "# Sample a batch\n",
    "batch = loaded_buffer.sample(batch_size=32)\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  States: {batch['states'].shape}\")\n",
    "print(f\"  Actions: {batch['actions'].shape}\")\n",
    "print(f\"  Rewards: {batch['rewards'].shape}\")\n",
    "print(f\"  Next states: {batch['next_states'].shape}\")\n",
    "print(f\"  Dones: {batch['dones'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc759",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Behavior Policy Quality**: The dataset quality affects CQL performance\n",
    "2. **Dataset Size**: More data generally helps, but CQL can work with limited data\n",
    "3. **Action Coverage**: Important for offline RL - need diverse actions\n",
    "4. **Survival Rate**: Behavior policy survival rate sets a baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

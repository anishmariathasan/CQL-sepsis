{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.environments.icu_sepsis_wrapper import create_sepsis_env, ICUSepsisWrapper\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723eac4",
   "metadata": {},
   "source": [
    "## 1. Environment Overview\n",
    "\n",
    "The ICU-Sepsis environment simulates sepsis treatment in an ICU setting:\n",
    "- **States**: 716 discrete states representing patient condition\n",
    "- **Actions**: 25 discrete actions (5 vasopressor levels × 5 IV fluid levels)\n",
    "- **Rewards**: Sparse (+1 survival, -1 death, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = create_sepsis_env(use_action_masking=True)\n",
    "\n",
    "print(\"Environment Details:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Number of states: {env.n_states}\")\n",
    "print(f\"  Number of actions: {env.n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657bca4",
   "metadata": {},
   "source": [
    "## 2. Action Space Analysis\n",
    "\n",
    "Actions are combinations of:\n",
    "- Vasopressor dose (0-4)\n",
    "- IV fluid volume (0-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action decomposition\n",
    "print(\"Action Space (Vasopressor × IV Fluid):\")\n",
    "print(\"\\nAction ID | Vasopressor | IV Fluid\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for action in range(25):\n",
    "    vaso = action // 5\n",
    "    iv = action % 5\n",
    "    print(f\"    {action:2d}    |      {vaso}      |    {iv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d15569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise action grid\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "action_grid = np.arange(25).reshape(5, 5)\n",
    "im = ax.imshow(action_grid, cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('IV Fluid Level')\n",
    "ax.set_ylabel('Vasopressor Level')\n",
    "ax.set_title('Action Space (25 Discrete Actions)')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_xticklabels(['None', 'Low', 'Med', 'High', 'V.High'])\n",
    "ax.set_yticklabels(['None', 'Low', 'Med', 'High', 'V.High'])\n",
    "\n",
    "plt.colorbar(im, label='Action Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be8537",
   "metadata": {},
   "source": [
    "## 3. Running Episodes\n",
    "\n",
    "Let's run some episodes with a random policy to understand the environment dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23787c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy='random', seed=None):\n",
    "    \"\"\"Run a single episode and collect trajectory.\"\"\"\n",
    "    if seed is not None:\n",
    "        state, info = env.reset(seed=seed)\n",
    "    else:\n",
    "        state, info = env.reset()\n",
    "    \n",
    "    trajectory = {\n",
    "        'states': [state],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        if policy == 'random':\n",
    "            action = env.action_space.sample()\n",
    "        elif policy == 'no_treatment':\n",
    "            action = 0  # No vasopressor, no IV\n",
    "        elif policy == 'max_treatment':\n",
    "            action = 24  # Max vasopressor, max IV\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        trajectory['actions'].append(action)\n",
    "        trajectory['rewards'].append(reward)\n",
    "        trajectory['states'].append(next_state)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    return trajectory, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaa54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple episodes with random policy\n",
    "n_episodes = 100\n",
    "returns = []\n",
    "lengths = []\n",
    "outcomes = []  # 1 for survival, 0 for death\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    trajectory, total_reward = run_episode(env, policy='random', seed=i)\n",
    "    returns.append(total_reward)\n",
    "    lengths.append(len(trajectory['actions']))\n",
    "    outcomes.append(1 if total_reward > 0 else 0)\n",
    "\n",
    "print(f\"Random Policy Statistics ({n_episodes} episodes):\")\n",
    "print(f\"  Survival rate: {np.mean(outcomes):.1%}\")\n",
    "print(f\"  Mean return: {np.mean(returns):.3f} ± {np.std(returns):.3f}\")\n",
    "print(f\"  Mean episode length: {np.mean(lengths):.1f} ± {np.std(lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise episode statistics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Return distribution\n",
    "axes[0].hist(returns, bins=3, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Episode Return')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Return Distribution (Random Policy)')\n",
    "\n",
    "# Episode length distribution\n",
    "axes[1].hist(lengths, bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Episode Length')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Episode Length Distribution')\n",
    "\n",
    "# Survival rate pie chart\n",
    "survival_counts = [sum(outcomes), len(outcomes) - sum(outcomes)]\n",
    "axes[2].pie(survival_counts, labels=['Survived', 'Died'], autopct='%1.1f%%',\n",
    "            colors=['green', 'red'], explode=[0.05, 0])\n",
    "axes[2].set_title('Patient Outcomes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497e483",
   "metadata": {},
   "source": [
    "## 4. Comparing Different Policies\n",
    "\n",
    "Let's compare different simple policies to understand the environment better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76226d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \"\"\"Evaluate a policy over multiple episodes.\"\"\"\n",
    "    returns = []\n",
    "    for i in range(n_episodes):\n",
    "        _, total_reward = run_episode(env, policy=policy, seed=i)\n",
    "        returns.append(total_reward)\n",
    "    \n",
    "    survival_rate = np.mean([r > 0 for r in returns])\n",
    "    return {\n",
    "        'mean_return': np.mean(returns),\n",
    "        'std_return': np.std(returns),\n",
    "        'survival_rate': survival_rate,\n",
    "    }\n",
    "\n",
    "policies = ['random', 'no_treatment', 'max_treatment']\n",
    "results = {}\n",
    "\n",
    "for policy in policies:\n",
    "    results[policy] = evaluate_policy(env, policy, n_episodes=100)\n",
    "    print(f\"{policy}: Survival={results[policy]['survival_rate']:.1%}, \"\n",
    "          f\"Return={results[policy]['mean_return']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a28b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise policy comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "policy_names = list(results.keys())\n",
    "survival_rates = [results[p]['survival_rate'] for p in policy_names]\n",
    "\n",
    "bars = ax.bar(policy_names, survival_rates, color=['blue', 'red', 'green'], alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Policy', fontsize=12)\n",
    "ax.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax.set_title('Policy Comparison: Survival Rates', fontsize=14)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, survival_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{rate:.1%}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee163856",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "From this exploration, we learned:\n",
    "\n",
    "1. **Environment Structure**: The ICU-Sepsis environment has 716 discrete states and 25 actions\n",
    "2. **Sparse Rewards**: Only terminal rewards (+1 survival, -1 death)\n",
    "3. **Baseline Performance**: Random policy achieves ~50-60% survival (varies by dataset)\n",
    "4. **Challenge**: Need to learn effective treatment policies from offline data\n",
    "\n",
    "Next steps:\n",
    "- Collect offline data with behavior policies\n",
    "- Train CQL agents on the offline data\n",
    "- Compare against baselines"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
